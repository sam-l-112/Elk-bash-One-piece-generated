#!/usr/bin/env bash
set -euo pipefail

# --------------------------------------------
# ðŸ“¦ Oneâ€‘Stop Installer: k3s + ELK + Filebeat + Data Pipeline (with API Key from create_api_key.sh)
# --------------------------------------------

# === Phase 1: k3s + ELK Base Deployment ===
echo "ðŸ”§ Clone and setup Ansible/k3s"
git clone https://github.com/DevSecOpsLab-CSIE-NPU/AQUA-CARE-2025-June || true
cd AQUA-CARE-2025-June

bash tools/install_ansbile.sh
source .venv/bin/activate
pip install ansible requests joblib tqdm
ansible-playbook -i ansible/inventories/hosts.ini ansible/playbooks/install_k3s.yaml

# Configure kubeconfig
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown "$(id -u):$(id -g)" ~/.kube/config
chmod 600 ~/.kube/config
export KUBECONFIG=$HOME/.kube/config
echo 'export KUBECONFIG=$HOME/.kube/config' >> ~/.bashrc
source ~/.bashrc
kubectl get po -A

# === Phase 2: Install ELK ===
echo "ðŸ“¦ Deploy ELK via Helm"
cd elk/
helm repo add elastic https://helm.elastic.co || true
helm repo update

for chart in elasticsearch filebeat logstash kibana; do
  if helm list | grep -q "$chart"; then
    echo "âš ï¸ $chart is already installed. Skipping."
  else
    echo "â¬†ï¸ Installing $chart ..."
    helm install $chart elastic/$chart -f $chart/values.yml
  fi
done

kubectl get all -n default

# === Phase 3: Password ===
HOST_IP=$(hostname -I | awk '{print $1}')
HOST_NAME=$(hostname)
echo "ðŸ“¡ æœ¬æ©Ÿåç¨±: $HOST_NAME"
echo "ðŸŒ æœ¬æ©Ÿ IP: $HOST_IP"

ELASTIC_PASS=$(kubectl get secret elasticsearch-master-credentials \
  -o jsonpath="{.data.password}" | base64 --decode)
echo "â†’ elastic å¸³è™Ÿå¯†ç¢¼: $ELASTIC_PASS"

# === Phase 4: Filebeat Install & Config ===
echo "ðŸ“¥ Install Filebeat on host"
curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | \
  sudo gpg --dearmor -o /usr/share/keyrings/elastic-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/elastic-keyring.gpg] https://artifacts.elastic.co/packages/9.x/apt stable main" | \
  sudo tee /etc/apt/sources.list.d/elastic-9.x.list > /dev/null

sudo apt-get update
sudo apt-get install -y filebeat
sudo systemctl enable filebeat

# === Phase 5: Import Data Pipeline and Auto Extract API Key ===
echo "ðŸ”„ Import sample data"
cd elasticsearch

bash go.sh || echo "âš ï¸ go.sh åŸ·è¡Œå¤±æ•—"
bash create_api_key.sh > api_key_output.json

ENCODED_KEY=$(grep -oP '"encoded"\s*:\s*"\K[^"]+' api_key_output.json | tail -n 1)
if [[ -z "$ENCODED_KEY" ]]; then
  echo "âŒ ç„¡æ³•è‡ªå‹•æ“ä½œ API Key, è«‹æ‰‹å‹•åŸ·è¡Œ create_api_key.sh"
  exit 1
fi

echo "ðŸ” Extracted API Key: $ENCODED_KEY"

sudo tee /etc/filebeat/filebeat.yml > /dev/null <<EOF
output.elasticsearch:
  hosts: ["https://localhost:9200"]
  api_key: "$ENCODED_KEY"
  ssl.verification_mode: "none"
EOF

sudo filebeat test config
sudo filebeat test output
sudo systemctl restart filebeat

bash test_api_key.sh || echo "âš ï¸ test_api_key.sh å¤±æ•—"

read -rp "ðŸ“… æ˜¯å¦å•Ÿç”¨ dataset åŒ¯å…¥ï¼Ÿ(y/N): " IMPORT_DATA
if [[ $IMPORT_DATA == "y" || $IMPORT_DATA == "Y" ]]; then
  source ../../.venv/bin/activate
  python3 import_dataset.py
  echo "âœ… Dataset åŒ¯å…¥å®Œæˆ"
else
  echo "â­ï¸ å·²ç•¥éŽ dataset åŒ¯å…¥"
fi

echo "âœ¨ All setup complete! You can now use Kibana at: http://localhost:5601"
